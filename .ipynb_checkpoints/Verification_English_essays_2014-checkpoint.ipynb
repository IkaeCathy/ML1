{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length data11: 400 200 200\n",
      "676571\n",
      "Voc: 13217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import to_categorical\n",
    "import unicodedata\n",
    "import sys\n",
    "import re\n",
    "from sstemmer import *\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        \n",
    "def cleaningLinePAN(aLine):\n",
    "   if (aLine.find(\"<author\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   if (aLine.find(\"<conversation\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   if (aLine.find(\"</conversation\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   if (aLine.find(\"</author\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   if (aLine.find(\"<documents>\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   if (aLine.find(\"</documents>\") >= 0) :\n",
    "      aLine = \"\"\n",
    "   aLine = aLine.replace(\"</document>\", \"\")\n",
    "   aLine = aLine.replace(\"\\n\", \" \")\n",
    "   aLine = aLine.replace(\"&quot;\", '\"')\n",
    "   aLine = aLine.replace(\"&gt;\", \">\")\n",
    "   aLine = aLine.replace(\"&lt;\", \"<\")\n",
    "   aLine = aLine.replace(\"&nbsp;\", \" \")\n",
    "   aLine = aLine.replace(\"&amp;\", \"&\")\n",
    "   aLine = aLine.replace(\"<>\", \" \")\n",
    "   aLine = aLine.replace(\"[...]\", \"\")\n",
    "   aLine = aLine.replace(\"<![CDATA[\", \"\")\n",
    "   aLine = aLine.replace(\"]]>\", \"\")\n",
    "   aLine = aLine.replace(\"  \", \" \")\n",
    "   if (len(aLine) > 1):\n",
    "      aLine = aLine.replace(\"\\t\",\" \")\n",
    "   if (len(aLine) < 1):\n",
    "      return(\"\")\n",
    "\n",
    "   aMatch = re.search(\"<br[ ]?[/]?>\", aLine)\n",
    "   while (aMatch):\n",
    "      aLine = aLine[:aMatch.start()] + \" \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"<br[ ]?[/]?>\", aLine)\n",
    "      \n",
    "   aMatch = re.search(\"<a .*?>\", aLine)\n",
    "   while (aMatch):\n",
    "      aLine = aLine[:aMatch.start()]+ \" AREF \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"<a[.]*>\", aLine)\n",
    "      \n",
    "   aMatch = re.search(\"<.*?>\", aLine)\n",
    "   while (aMatch):\n",
    "#     print \"<TAGS\", aMatch.group(0), aMatch.start(), aMatch.end()\n",
    "      aLine = aLine[:aMatch.start()]+ \" \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"<.*?>\", aLine)\n",
    "         \n",
    "   aMatch = re.search(\"\\&.*?;\", aLine)\n",
    "   while (aMatch):\n",
    "#      print \"<CODE\", aMatch.group(0), aMatch.start(), aMatch.end()\n",
    "      aLine = aLine[:aMatch.start()] + \" \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"\\&.*?;\", aLine)\n",
    "# Coded character:  If the string  aLine is in UTF-8 nothing is modified     \n",
    "   aMatch = re.search(r\"\\\\u\\d{3,4}\", aLine, re.I)\n",
    "   while (aMatch):\n",
    "#      print \"Code U\" , aMatch.group(0), aMatch.start(), aMatch.end()\n",
    "      aLine = aLine[:aMatch.start()-1] + \" \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"\\\\u\\d{3,4}\", aLine, re.I)\n",
    "\n",
    "# http://anURL    \n",
    "   aMatch = re.search(\"http[s]?://[A-Za-z0-9/\\.]* ?\", aLine, re.I)\n",
    "   while (aMatch):\n",
    "#    print \"http\" , aMatch.group(0), aMatch.start(), aMatch.end()\n",
    "      aPos = max(aMatch.start()-1, 0)\n",
    "      aLine = aLine[:aPos] + \" urllink \" + aLine[aMatch.end():]\n",
    "      aMatch = re.search(\"http[s]?://[A-Za-z0-9/\\.]* ?\", aLine)\n",
    "      \n",
    "   aLine = aLine.replace(\"  \", \" \")\n",
    "   if (aLine == \" \"):\n",
    "      aLine = \"\"\n",
    "   return(aLine)\n",
    "\n",
    "##create feature vector from the extracted token list.....................................................................................\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "    fvs_words = np.array([(all_training_text.count(word))/(len(all_training_text)) for word in word_list]).astype(np.float64)\n",
    "    return fvs_words\n",
    "\n",
    "def WordFeatures1(word_list, all_training_text):\n",
    "            fvs_words = np.array([[author.count(word) for word in word_list]\n",
    "                                   for author in all_training_text]).astype(np.float64)\n",
    "            # normalise by dividing each row by number of tokens for each author........\n",
    "            fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "            return fvs_words\n",
    "        \n",
    "        \n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "          unicode = str\n",
    "def tokenizeLineUTF8(newLine):\n",
    "   aListChar = []\n",
    "   aListCode = []\n",
    "   for i, c in enumerate(newLine):\n",
    "      aListCode.append(unicodedata.category(c)[0])\n",
    "      aListChar.append(c)\n",
    "   seqChar = seqPunct = seqNumber = False\n",
    "   aLen = len(aListCode)\n",
    "   aToken = u''\n",
    "   aListOfWord = []\n",
    "   for anIndex in range(aLen):\n",
    "      aCode = aListCode[anIndex]\n",
    "      aChar = aListChar[anIndex]\n",
    "      if ((aCode == \"L\") & (seqChar)):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"P\") & ((seqPunct) | (seqNumber))):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"N\") & ((seqPunct) | (seqNumber))):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"Z\") | (aCode == \"C\") | (aCode == \"M\") | (aCode == \"S\")):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = u''\n",
    "         seqChar = seqPunct = seqNumber = False\n",
    "      elif ((aCode == \"L\") & (not seqChar)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqChar = True\n",
    "         seqPunct = seqNumber = False\n",
    "      elif ((aCode == \"P\") & (not seqPunct)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqPunct = True\n",
    "         seqChar = seqNumber = False\n",
    "      elif ((aCode == \"N\") & (not seqNumber)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqNumber = True\n",
    "         seqChar = seqPunct = False\n",
    "   if (len(aToken) > 0):\n",
    "      aListOfWord.append(aToken)\n",
    "   return(aListOfWord)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calcC1(truthDict, answerDict):\n",
    "    correct = 0.0\n",
    "    incorrect = 0.0\n",
    "    for e in range(len(truthDict)):\n",
    "        if truthDict[e] == 1:\n",
    "            if answerDict[e] > 0.5:\n",
    "                correct += 1\n",
    "            elif answerDict[e] < 0.5:\n",
    "                incorrect += 1\n",
    "        if truthDict[e] == 0:\n",
    "            if answerDict[e] < 0.5:\n",
    "                correct += 1\n",
    "            elif answerDict[e] > 0.5:\n",
    "                incorrect += 1\n",
    "    n = len(truthDict)\n",
    "    unknown = len([x for x in answerDict if x == 0.5])\n",
    "    return (1.0/n)*(correct+(unknown*correct/n))\n",
    "\n",
    "def feature_vect1(INPUT_DIRECTORY):\n",
    "    data11=[]\n",
    "    data22=[]\n",
    "    with open(INPUT_DIRECTORY +os.sep+ 'contents.json','r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        dict_items = data.items()\n",
    "        list_items = list(dict_items)\n",
    "        en_files =(list_items[2][1])\n",
    "        entries = sorted(os.listdir(INPUT_DIRECTORY))\n",
    "\n",
    "        for i in range(len(en_files)):\n",
    "            known_sp = []\n",
    "            files = os.listdir(INPUT_DIRECTORY +os.sep+en_files[i])\n",
    "            \n",
    "            known = [filename for filename in files if filename.startswith(\"known\")]\n",
    "            for j in known:\n",
    "                with open(INPUT_DIRECTORY +os.sep+en_files[i]+os.sep+j) as f_0:\n",
    "                    contents_0 = f_0.read()\n",
    "                    known_sp.append(contents_0.replace('\\n', ''))\n",
    "            data11.append(''.join(known_sp))\n",
    "                    \n",
    "            unknown = [filename for filename in files if filename.startswith(\"unknown\")]\n",
    "            with open(INPUT_DIRECTORY +os.sep+en_files[i]+os.sep+unknown[0]) as f_1:\n",
    "                contents_1 = f_1.read()\n",
    "                data22.append(contents_1.replace('\\n', ''))\n",
    "    return data11, data22\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "def preprocess_text(text):\n",
    "    \"\"\" A function to lower and tokenize text data \"\"\" \n",
    "    # Lower the text\n",
    "    lower_text = cleaningLinePAN(text.lower())\n",
    "\n",
    "    # tokenize the text into a list of words\n",
    "    #tokens = nltk.tokenize.word_tokenize(lower_text)\n",
    "    tokens = tokenizeLineUTF8(lower_text)\n",
    "    tokens =[WordNet_lemmatizer.lemmatize(word,'v') for word in tokens]\n",
    "    tokens = [SStemmer().stem(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "TRAIN_DIRECTORY= \"/Users/catherine/Desktop/NLP/PAN_Datasets/Verification/veri_2014/pan14-authorship-verification-test-and-training/pan14-authorship-verification-training-corpus-2014-04-22/pan14-author-verification-training-corpus-english-essays-2014-04-22\"\n",
    "TEST_DIRECTORY= \"/Users/catherine/Desktop/NLP/PAN_Datasets/Verification/veri_2014/pan14-authorship-verification-test-and-training/pan14-authorship-verification-test-corpus1-2014-04-22/pan14-author-verification-test-corpus1-english-essays-2014-04-22\"\n",
    "\n",
    "data11 = feature_vect1(TRAIN_DIRECTORY)[0]\n",
    "data22 = feature_vect1(TRAIN_DIRECTORY)[1]\n",
    "data= data11 + data22\n",
    "print(\"Length data11:\",len(data11 + data22), len(data22), len(data11))\n",
    "# Final list with tokenized words\n",
    "tokenized_final = []\n",
    "\n",
    "# Iterating over each string in data\n",
    "for x in data:\n",
    "    # Calliing preprocess text function\n",
    "    token = preprocess_text(x)\n",
    "\n",
    "    tokenized_final.append(token) \n",
    "\n",
    "flattened_tokeninized_final = [i for j in tokenized_final for i in j]\n",
    "print(len(flattened_tokeninized_final))\n",
    "fdist = nltk.FreqDist(flattened_tokeninized_final)\n",
    "fdist = sorted(dict(fdist).items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Voc:\", len(fdist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Tokens 13217\n",
      "len word_list 200\n",
      "(X_train_0.shape) (200, 200)\n",
      "(X_train_1.shape) (200, 200)\n",
      "(X_train.shape) (200, 200)\n",
      "y_train.shape (200,) {0, 1} [0 1]\n",
      "100 100\n",
      "(X_test_0.shape) (100, 200)\n",
      "(X_test_1.shape) (100, 200)\n",
      "(X_test.shape) (100, 200)\n",
      "len y_test 100\n",
      "(100,) {0, 1} [0 1] \n",
      "\n",
      "GradientBoostingClassifier()\n",
      "accuracy:  0.6\n",
      "AUC:  0.6732\n",
      "FINAL1: 0.404 \n",
      "\n",
      "AUC: 0.674\n",
      "c@1: 0.614\n",
      "FINAL2: 0.413 \n",
      "\n",
      "Accuracy: 0.6\n",
      "Auc: 0.67\n",
      "c@1: 0.600\n",
      "FINAL3: 0.404 \n",
      "\n",
      "KNeighborsClassifier()\n",
      "accuracy:  0.6\n",
      "AUC:  0.6112\n",
      "FINAL1: 0.367 \n",
      "\n",
      "AUC: 0.611\n",
      "c@1: 0.600\n",
      "FINAL2: 0.367 \n",
      "\n",
      "Accuracy: 0.6\n",
      "Auc: 0.61\n",
      "c@1: 0.600\n",
      "FINAL3: 0.367 \n",
      "\n",
      "LinearDiscriminantAnalysis()\n",
      "accuracy:  0.55\n",
      "AUC:  0.548\n",
      "FINAL1: 0.301 \n",
      "\n",
      "AUC: 0.548\n",
      "c@1: 0.550\n",
      "FINAL2: 0.301 \n",
      "\n",
      "Accuracy: 0.55\n",
      "Auc: 0.55\n",
      "c@1: 0.550\n",
      "FINAL3: 0.301 \n",
      "\n",
      "AdaBoostClassifier()\n",
      "accuracy:  0.59\n",
      "AUC:  0.6044\n",
      "FINAL1: 0.357 \n",
      "\n",
      "AUC: 0.546\n",
      "c@1: 0.168\n",
      "FINAL2: 0.092 \n",
      "\n",
      "Accuracy: 0.59\n",
      "Auc: 0.6\n",
      "c@1: 0.590\n",
      "FINAL3: 0.357 \n",
      "\n",
      "RandomForestClassifier()\n",
      "accuracy:  0.62\n",
      "AUC:  0.6645999999999999\n",
      "FINAL1: 0.426 \n",
      "\n",
      "AUC: 0.646\n",
      "c@1: 0.550\n",
      "FINAL2: 0.355 \n",
      "\n",
      "Accuracy: 0.62\n",
      "Auc: 0.66\n",
      "c@1: 0.640\n",
      "FINAL3: 0.426 \n",
      "\n",
      "MLPClassifier()\n",
      "accuracy:  0.5\n",
      "AUC:  0.6428\n",
      "FINAL1: 0.321 \n",
      "\n",
      "AUC: 0.500\n",
      "c@1: 0.000\n",
      "FINAL2: 0.000 \n",
      "\n",
      "Accuracy: 0.5\n",
      "Auc: 0.64\n",
      "c@1: 0.500\n",
      "FINAL3: 0.321 \n",
      "\n",
      "SVC(probability=True)\n",
      "accuracy:  0.62\n",
      "AUC:  0.6796\n",
      "FINAL1: 0.344 \n",
      "\n",
      "AUC: 0.500\n",
      "c@1: 0.000\n",
      "FINAL2: 0.000 \n",
      "\n",
      "Accuracy: 0.62\n",
      "Auc: 0.68\n",
      "c@1: 0.506\n",
      "FINAL3: 0.344 \n",
      "\n",
      "SGDClassifier(loss='log')\n",
      "accuracy:  0.5\n",
      "AUC:  0.6688\n",
      "FINAL1: 0.334 \n",
      "\n",
      "AUC: 0.500\n",
      "c@1: 0.000\n",
      "FINAL2: 0.000 \n",
      "\n",
      "Accuracy: 0.5\n",
      "Auc: 0.67\n",
      "c@1: 0.500\n",
      "FINAL3: 0.334 \n",
      "\n",
      "ExtraTreesClassifier()\n",
      "accuracy:  0.65\n",
      "AUC:  0.7220000000000001\n",
      "FINAL1: 0.488 \n",
      "\n",
      "AUC: 0.703\n",
      "c@1: 0.620\n",
      "FINAL2: 0.436 \n",
      "\n",
      "Accuracy: 0.65\n",
      "Auc: 0.72\n",
      "c@1: 0.676\n",
      "FINAL3: 0.488 \n",
      "\n",
      "LogisticRegression()\n",
      "accuracy:  0.6\n",
      "AUC:  0.6712\n",
      "FINAL1: 0.403 \n",
      "\n",
      "AUC: 0.500\n",
      "c@1: 0.000\n",
      "FINAL2: 0.000 \n",
      "\n",
      "Accuracy: 0.6\n",
      "Auc: 0.67\n",
      "c@1: 0.600\n",
      "FINAL3: 0.403 \n",
      "\n",
      "XGBClassifier()\n",
      "accuracy:  0.62\n",
      "AUC:  0.6692\n",
      "FINAL1: 0.415 \n",
      "\n",
      "AUC: 0.670\n",
      "c@1: 0.630\n",
      "FINAL2: 0.422 \n",
      "\n",
      "Accuracy: 0.62\n",
      "Auc: 0.67\n",
      "c@1: 0.620\n",
      "FINAL3: 0.415 \n",
      "\n",
      "MultinomialNB()\n",
      "accuracy:  0.55\n",
      "AUC:  0.6936\n",
      "FINAL1: 0.381 \n",
      "\n",
      "AUC: 0.500\n",
      "c@1: 0.000\n",
      "FINAL2: 0.000 \n",
      "\n",
      "Accuracy: 0.55\n",
      "Auc: 0.69\n",
      "c@1: 0.550\n",
      "FINAL3: 0.381 \n",
      "\n",
      "GaussianNB()\n",
      "accuracy:  0.62\n",
      "AUC:  0.6886\n",
      "FINAL1: 0.427 \n",
      "\n",
      "AUC: 0.689\n",
      "c@1: 0.620\n",
      "FINAL2: 0.427 \n",
      "\n",
      "Accuracy: 0.62\n",
      "Auc: 0.69\n",
      "c@1: 0.620\n",
      "FINAL3: 0.427 \n",
      "\n",
      "BernoulliNB()\n",
      "accuracy:  0.67\n",
      "AUC:  0.7192\n",
      "FINAL1: 0.482 \n",
      "\n",
      "AUC: 0.719\n",
      "c@1: 0.686\n",
      "FINAL2: 0.494 \n",
      "\n",
      "Accuracy: 0.67\n",
      "Auc: 0.72\n",
      "c@1: 0.670\n",
      "FINAL3: 0.482 \n",
      "\n",
      "VotingClassifier(estimators=[('GradientBoostingClassifier',\n",
      "                              GradientBoostingClassifier()),\n",
      "                             (' ExtraTreesClassifier', ExtraTreesClassifier()),\n",
      "                             ('RandomForestClassifier',\n",
      "                              RandomForestClassifier())],\n",
      "                 voting='soft')\n",
      "accuracy:  0.62\n",
      "AUC:  0.6876\n",
      "FINAL1: 0.426 \n",
      "\n",
      "AUC: 0.687\n",
      "c@1: 0.617\n",
      "FINAL2: 0.424 \n",
      "\n",
      "Accuracy: 0.62\n",
      "Auc: 0.69\n",
      "c@1: 0.620\n",
      "FINAL3: 0.426 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "word_list=list(dict(fdist).keys())\n",
    "l=0.2*(len(word_list))\n",
    "i=int(l)\n",
    "print(\"len Tokens\", len(word_list))\n",
    "word_list = word_list[0:200]\n",
    "print(\"len word_list\", len(word_list))\n",
    "\n",
    "X_train_0 = WordFeatures1(word_list, data11)\n",
    "X_train_1 = WordFeatures1(word_list, data22)\n",
    "print(\"(X_train_0.shape)\",(X_train_0.shape))\n",
    "print(\"(X_train_1.shape)\",(X_train_1.shape))\n",
    "#X_train =np.concatenate((X_train_0, X_train_1),axis=1)\n",
    "X_train=np.abs(X_train_0-X_train_1) #difference vector\n",
    "print(\"(X_train.shape)\",(X_train.shape))\n",
    "\n",
    "truth = pd.read_csv(TRAIN_DIRECTORY+os.sep+'truth.txt', sep=\" \", header=None)\n",
    "y_train = np.array(truth[truth.columns[1]])\n",
    "label_encoder = LabelEncoder()\n",
    "y_train =label_encoder.fit_transform(y_train)\n",
    "classes = np.unique((np.array(y_train)))\n",
    "print(\"y_train.shape\",y_train.shape, set(y_train), classes )\n",
    "\n",
    "\n",
    "data_test11 = feature_vect1(TEST_DIRECTORY)[0]\n",
    "data_test22 = feature_vect1(TEST_DIRECTORY)[1]\n",
    "print(len(data_test11), len(data_test22))\n",
    "\n",
    "X_test_0 = WordFeatures1(word_list, data_test11)\n",
    "X_test_1 = WordFeatures1(word_list, data_test22)\n",
    "#X_test =np.concatenate((X_test_0, X_test_1),axis=1)\n",
    "print(\"(X_test_0.shape)\",(X_test_0.shape))\n",
    "print(\"(X_test_1.shape)\",(X_test_1.shape))\n",
    "X_test=np.abs(X_test_0-X_test_1) #difference vector\n",
    "print(\"(X_test.shape)\",(X_test.shape))\n",
    "\n",
    "\n",
    "truth_test = pd.read_csv(TEST_DIRECTORY+os.sep+'truth.txt', sep=\" \", header=None)\n",
    "y_test = np.array(truth_test[truth_test.columns[1]])\n",
    "print(\"len y_test\",len(y_test))\n",
    "label_encoder = LabelEncoder()\n",
    "y_test =label_encoder.fit_transform(y_test)\n",
    "classes = np.unique((np.array(y_test)))\n",
    "print(y_test.shape, set(y_test), classes , \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "ensemble_GB_RF_ET=VotingClassifier(estimators= [('GradientBoostingClassifier', GradientBoostingClassifier()),\n",
    "                                                    (' ExtraTreesClassifier',  ExtraTreesClassifier()),\n",
    "                                                    #('AdaBoostClassifier', AdaBoostClassifier()),\n",
    "                                                    ('RandomForestClassifier', RandomForestClassifier()),\n",
    "                                                    #('SVC', SVC(probability=True)),\n",
    "                                                    #('LogisticRegression()', LogisticRegression())\n",
    "                                                       ], voting='soft')\n",
    "\n",
    "ensemble_GrBoost_xgb=[ GradientBoostingClassifier(),KNeighborsClassifier(), \n",
    "                                                    LinearDiscriminantAnalysis(),\n",
    "                                                    AdaBoostClassifier(),\n",
    "                                                    RandomForestClassifier(),\n",
    "                                                    MLPClassifier(),\n",
    "                                                    SVC(probability=True),\n",
    "                                                    SGDClassifier(loss='log'),\n",
    "                                                    ExtraTreesClassifier(),\n",
    "                                                    LogisticRegression(),\n",
    "                                                    XGBClassifier(),\n",
    "                                                    MultinomialNB(),GaussianNB(),BernoulliNB(),\n",
    "                                                       ensemble_GB_RF_ET]\n",
    "        \n",
    "\n",
    "for model in ensemble_GrBoost_xgb:\n",
    "            print(model)\n",
    "        \n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            predicted_prob = model.predict_proba(X_test)\n",
    "            accuracy = metrics.accuracy_score(y_test, preds)\n",
    "            ca1 = calcC1(y_test, predicted_prob[:, 1])\n",
    "            fpr, tpr, thresh = roc_curve(y_test, predicted_prob[:, 1])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print('accuracy: ', accuracy)\n",
    "            print('AUC: ', roc_auc)\n",
    "            FINAL1=roc_auc*ca1\n",
    "            print('FINAL1: %.3f' % FINAL1, \"\\n\")\n",
    "            \n",
    "\n",
    "            \n",
    "            predicted_prob = model.predict_proba(X_test)\n",
    "            answer=(predicted_prob[:, 1])\n",
    "            answer_new=[0.5 if 0.45< a_<0.55 else a_ for a_ in answer]\n",
    "            ca1 = calcC1(y_test, answer_new)\n",
    "            auc1 = metrics.roc_auc_score(y_test, answer_new)\n",
    "            FINAL2=auc1*ca1\n",
    "            print('AUC: %.3f' % auc1)\n",
    "            print(\"c@1: %.3f\" % ca1)\n",
    "            print('FINAL2: %.3f' % FINAL2, \"\\n\")\n",
    "            \n",
    "            predicted = model.predict(X_test)\n",
    "            predicted_prob = model.predict_proba(X_test)\n",
    "            classes = np.unique(y_test)\n",
    "            y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "            accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "            auc2 = metrics.roc_auc_score(y_test, predicted_prob[:, 1])\n",
    "            ca1 = calcC1(y_test, answer)\n",
    "            FINAL3=auc2*ca1\n",
    "            print(\"Accuracy:\",  round(accuracy,2))\n",
    "            print(\"Auc:\", round(auc2,2))\n",
    "            print(\"c@1: %.3f\" % ca1)\n",
    "            print('FINAL3: %.3f' % FINAL3, \"\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
